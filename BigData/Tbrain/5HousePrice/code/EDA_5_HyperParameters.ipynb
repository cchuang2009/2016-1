{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA, Exploratory Data Analysis\n",
    "===\n",
    "Hyper-Parameter Optimization\n",
    "---\n",
    "As well-known fact, suitable tune parameter could get better result; here, we will introduce how to tune hyperparameters of machine learning algorithm in the most effective way.\n",
    "\n",
    "\n",
    "1. nan conversion\n",
    "2. target, $\\mathbf{y\\Rightarrow\\log(1+y)}$ (`np.log1p`), for normalize fitting; later, back by $\\mathbf{y_p \\Rightarrow \\exp(y_p)-1}$ (`np.expm1`); or use `np.log` directly if all the target data are greater than 0.\n",
    "- latitude/longitude conversion, aÂ°). knn means, bÂ°). dbscans, then one-hot converstion\n",
    "  ```Lasso, 0.7012 âž¡ï¸Ž 0.6893```, the last one can not assign a fixed value to fix the data.\n",
    "- different models, xgb, lgb, ...; here we try the `lightgbm`;\n",
    "- stack model, blend moder, ...; install `mlxtend` by pip.\n",
    "- <font color=\"red\">Hyper-parameter tuning</font>: standard approaches such as Grid Search and Random Search.\n",
    "\n",
    "Note\n",
    "---\n",
    "1. Since 2019/05/17, two new implementations of gradient boosting trees: `ensemble.HistGradientBoostingClassifier` and `ensemble.HistGradientBoostingRegressor` , are supported by `scikit_learning`-0.21.1.\n",
    "```python\n",
    "# usage\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "HistGradientBoostingRegressor(loss=â€™least_squaresâ€™, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_bins=256, scoring=None, validation_fraction=0.1, n_iter_no_change=None, tol=1e-07, verbose=0, random_state=None)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import folium\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train: \",train_df.shape[0],\"sales, and \",train_df.shape[1],\"features\")\n",
    "print (\"Test:  \",test_df.shape[0],\"sales, and \",test_df.shape[1],\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnan(df_1,df_2,features):\n",
    "    df_1[features[0]].fillna(value=0,inplace=True)\n",
    "    df_2[features[0]].fillna(value=0,inplace=True)\n",
    "    df_1[features[1]].fillna(value=0,inplace=True)\n",
    "    df_2[features[1]].fillna(value=0,inplace=True)\n",
    "    \n",
    "    df=pd.concat([df_1[features],df_2[features]],axis=0)\n",
    "    for f in features[2:]:\n",
    "        df_1[f].fillna(value=df[f].median(),inplace=True)\n",
    "        df_2[f].fillna(value=df[f].median(),inplace=True)\n",
    "    return df_1,df_2\n",
    "\n",
    "nan_features=['parking_area','parking_price','txn_floor','village_income_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of or replace nan\n",
    "train_df,test_df=fillnan(train_df,test_df,nan_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "---\n",
    "1. Quantitative\n",
    "   - time-state: 'txn_dt', 'building_complete_dt'\n",
    "   - non-time,\n",
    "           'parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_Min','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "   - location: 'lon','lat'\n",
    "- Qualitative\n",
    "  - building_material(9),city(11),total_floor(29),building_type(5),building_use(10),parking_way,\n",
    "    parking_area, txn_floor,'doc_rate', 'master_rate', 'bachelor_rate', 'jobschool_rate',\n",
    "       'highschool_rate', 'junior_rate', 'elementary_rate', 'born_rate',\n",
    "       'death_rate', 'marriage_rate', 'divorce_rate'\n",
    "  - village(2899)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = ['txn_dt', 'building_complete_dt','parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_MIN','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "           'lon','lat']\n",
    "qualitative=['building_material','city','total_floor','building_type','building_use',\n",
    "             'parking_way','parking_area','txn_floor','doc_rate', 'master_rate', \n",
    "             'bachelor_rate', 'jobschool_rate','highschool_rate', 'junior_rate', \n",
    "             'elementary_rate', 'born_rate','death_rate', 'marriage_rate', 'divorce_rate',\n",
    "             'village']\n",
    "target=['total_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_price_log']=np.log(train_df['total_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness and Kurtosis\n",
    "---\n",
    "\\begin{eqnarray}\n",
    "    \\text{Skewness }&=&\\frac{E(X-\\mu)^3}{\\sigma^3}\\\\\n",
    "    \\text{Kurtosis }&=&\\frac{E(X-\\mu)^4}{\\sigma^4}\n",
    "\\end{eqnarray}\n",
    "1. Skewness $>0$, log-tail on the right-side, and $<0$, on the left side,\n",
    "- Kurtosis large, steep in the peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that data should follow the `normal distributed` rule. Thus, we want to check the  quantitative features, continuous variables, whether obey the normal rule;\n",
    "```\n",
    "        convert the feature, with skew >1, by logarithm, ln(var);\n",
    "        translate the data if data is smaller than 0.\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "building_area              83.710631\n",
       "VIII_MIN                    8.002373\n",
       "XIII_MIN                    7.606963\n",
       "XII_MIN                     7.230123\n",
       "XI_MIN                      7.088242\n",
       "V_MIN                       5.753089\n",
       "IX_MIN                      5.401486\n",
       "parking_price               4.816163\n",
       "X_MIN                       4.719582\n",
       "VII_MIN                     4.632135\n",
       "III_MIN                     4.372702\n",
       "II_MIN                      4.077665\n",
       "village_income_median       2.848078\n",
       "I_MIN                       2.761933\n",
       "XIV_MIN                     2.680964\n",
       "IV_MIN                      2.428235\n",
       "VI_MIN                      2.257881\n",
       "town_area                   1.988336\n",
       "lat                         1.220302\n",
       "town_population_density     0.765919\n",
       "lon                         0.764275\n",
       "town_population             0.643185\n",
       "txn_dt                      0.158794\n",
       "building_complete_dt        0.030223\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check degree of skew \n",
    "skew_var_X={}\n",
    "for i in range(len(quantitative)):\n",
    "    skew_var_X[i]=abs(train_df[quantitative[i]].skew())\n",
    "skew_df=pd.Series(skew_var_X)#.sort_values(ascending=False)\n",
    "skew_df.index=quantitative\n",
    "skew_df.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_skew=train_df.copy()\n",
    "var_X_ln=skew_df.index[skew_df>1]\n",
    "for i in var_X_ln:\n",
    "    if min(train_df_skew[i])<=0:\n",
    "       train_df_skew[i]=np.log(train_df_skew[i]+abs(train_df_skew[i].min())+0.01) \n",
    "    else:\n",
    "       train_df_skew[i]=np.log(train_df_skew[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso($\\alpha$)\n",
    "---\n",
    "$$\\text{arg min}\\left(|| y-\\sum\\mathbf{\\beta\\cdot x}||+\\alpha\\sum|\\beta_i|\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model,X,y):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get the `better` alpha? Try one by one from $10^{-3}$ to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up train/valid data, X/y\n",
    "\n",
    "X=train_df_skew.drop(['building_id','total_price','total_price_log'],axis=1)\n",
    "y = train_df_skew['total_price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 0.001\n",
      "The r-square is 0.8917035438340241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_alphas = np.logspace(-3, 0, 100, base=10)\n",
    "lcv = LassoCV(alphas=lasso_alphas, cv=10) # Search the min MSE by CV\n",
    "lcv.fit(X, y)\n",
    "\n",
    "print('The best alpha is {}'.format(lcv.alpha_))\n",
    "print('The r-square is {}'.format(lcv.score(X, y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0001, random_state=1,tol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score (alpha = 0.00010): Î¼ 0.3804 (ðˆ 0.0036)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = (rmsle_cv(lasso, X,y))\n",
    "print(\"\\nLasso score (alpha = {:.5f}): Î¼ {:.4f} (ðˆ {:.4f})\\n\".format(0.0001,score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.001, random_state=1,tol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score (alpha = 0.00010): Î¼ 0.3841 (ðˆ 0.0036)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = (rmsle_cv(lasso, X,y))\n",
    "print(\"\\nLasso score (alpha = {:.5f}): Î¼ {:.4f} (ðˆ {:.4f})\\n\".format(0.0001,score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice \n",
    "---\n",
    "Ridge Model ($\\alpha$):\n",
    "$$\\text{arg min}\\left(|| y-\\sum\\mathbf{\\beta\\cdot x}||+\\alpha\\sum|\\beta_i|^2\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-2, 3, 100, base=10)\n",
    "\n",
    "# Search the min MSE by CV\n",
    "rcv = RidgeCV(alphas=alphas, store_cv_values=True) \n",
    "rcv.fit(X, y)\n",
    "\n",
    "\n",
    "print('The best alpha is {}'.format(rcv.alpha_))\n",
    "print('The r-square is {}'.format(rcv.score(X, y))) \n",
    "# Default score is rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_(y,yp):\n",
    "    hit=sum(np.abs((y-yp))/y<0.1)/len(y)\n",
    "    \n",
    "    score=np.round(hit,4)*1e4+(1-np.round(hit,4))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=Lasso(alpha=0.0001)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2477.7523"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp=clf.predict(X)\n",
    "\n",
    "yp1=np.exp(yp)\n",
    "y1=np.exp(y)\n",
    "score_(y1,yp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm, HyperParameter Tuning\n",
    "---\n",
    "We already watch out the power of lightgbm. Let ue to dig out more power by parameter tuning.\n",
    "\n",
    "Here, conisder the following parameters:\n",
    "\n",
    "- **Number of estimators**, number of boosting iterations, LightGBM is fairly robust to over-fitting so a large number usually results in better performance,\n",
    "- **Maximum depth**,  limits the number of nodes in the tree, used to avoid overfitting ( max_depth=-1  means unlimited depth),\n",
    "- **Number of leaves**, number of leaves in full tree,\n",
    "- **Subsample**, each model trained at each iteration will have only a specific percentage subset of observations,\n",
    "- **Column sampling by tree**, each model trained at each iteration will have only a specific percentage subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, \\\n",
    "                                     GridSearchCV, RandomizedSearchCV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = lgb.LGBMRegressor(objective='regression', \n",
    "                                       max_depth=8,\n",
    "                                       num_leaves=63,\n",
    "                                       subsample=0.8,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       colsample_bytree=0.8,\n",
    "                                       feature_fraction=1,#0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=1,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm: 0.2065 (0.0044)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lightgbm, X,y)\n",
    "print(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6574.3426"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm.fit(X,y)\n",
    "\n",
    "yp=lightgbm.predict(X)\n",
    "\n",
    "yp1=np.exp(yp)\n",
    "y1=np.exp(y)\n",
    "\n",
    "score_(y1,yp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_space = {'n_estimators': [1000, 1500, 2000, 2500],\n",
    "               'max_depth':  [4, 5, 8, -1],\n",
    "               'num_leaves': [15, 31, 63, 127],\n",
    "               'subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "               'colsample_bytree': [0.6, 0.7, 0.8, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "lgb_model = lgb.LGBMRegressor(boosting='gbdt', n_jobs=-1, random_state=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "lgb_cv = GridSearchCV(lgb_model, hyper_space, scoring='r2', cv=5, verbose=1)\n",
    "lgb_cv_results = lgb_cv.fit(X_train, y_train)\n",
    "print(\"BEST PARAMETERS: \" + str(lgb_cv_results.best_params_))\n",
    "print(\"BEST CV SCORE: \" + str(lgb_cv_results.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict (after fitting GridSearchCV is an estimator with best parameters)\n",
    "y_pred = lgb_cv.predict(X_test)\n",
    " \n",
    "# Score\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"R2 SCORE ON TEST DATA: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice\n",
    "---\n",
    "\n",
    "Complete the test stage and submit your score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
